GreenPlum
Column oriented tables
Array columns
Table distribution randomly - round robin, even
Pick column, but one value might occur more skewing the distribution so the overall processing time will be the proc time of the slowest node
Plus alter tables are costly
Plus new node, redistribute data
Plus vacuum of tables

Apache HAWQ
Apache Hadoop Native SQL
MadLib - Machine Learning libraries
Hadoop Native: from storage (HDFS), resource management (YARN) to deployment (Ambari). Integrates with Yarn
UDF - user defined function: Python

NameNode - in a rack stored metadata about HDFS. One dedicated machine runs it. It has the Image in memory
Secondary NameNode applies changes (log) to Image file and sends it back to the primary NameNode 
DataNode - data about data blocks. Many machines run this process
Blocks are replicated in 1:3

ResourceManager - It is a cluster level (one for each cluster) component and runs on the master machine. It manages resources and schedule applications running on top of YARN
NodeManager - It is a node level component (one on each node) and runs on each slave machine. It is responsible for managing containers and monitoring resource utilization in each container. It also keeps track of node health and log management. It continuously communicates with ResourceManager
MapReduce -  is the core component of processing in a Hadoop Ecosystem as it provides the logic of processing. In other words, MapReduce is a software framework which helps in writing applications that processes large data sets using distributed and parallel algorithms inside Hadoop environment. In a MapReduce program, Map() and Reduce() are two functions. The Map function performs actions like filtering, grouping and sorting. While Reduce function aggregates and summarizes the result 

hdfs dfs ls -l # Command line. Returns nothing as the home directory on HDFS is empty
hdfs dfs -put /home/hduser/input.txt /user/hduser # Copies a file from local file system to HDFS home
hdfs dfs -get input.txt /home/hduser # Other way around

Snakebite - is a Python library to communicate with the HDFS NameNode
uses Hadoop RPC to comm. No need for system calls with Hadoop dfs

client = Client('localhost', 9000)
for x in client.ls(['/']):
print x
# Returns directories for each file or directory with lots of key-value pairs describing the dir or file

hadoop/conf/core-site.xml holds all the parameters needed for Client

for p in client.mkdir(['/foo/bar', '/input'], create_parent=True):
print p
# Creates directories plus foo parent if does not exist
for p in client.delete(['/foo', '/input'], recurse=True):
print p
# Deletes dirs and sub-dires recursively
for f in client.copyToLocal(['/input/input.txt'], '/tmp'):
print f
# Copy to local /tmp from HDFS
for l in client.text(['/input/input.txt']):
print l
# Same as cat

CLI client
~/.snakebiterc
{
"config_version": 2,
"skiptrash": true,
"namenodes": [
{"host": "localhost", "port": 9000, "version": 9},
]
}

snakebite ls /

MapReduce
Input -> key:value pairs = Mapping
Shuffling -> partitioning + sorting. Hash sort so that each reducer gets a partitioned set
Reducer -> aggregates the result

Mapper:
for line in sys.stdin:
words = line.split()
# The key is anything before the first tab character and the value is anything after the first tab character.
print '{0}\t{1}'.format(word, 1)

Reducer:
import sys
curr_word = None
curr_count = 0

for line in sys.stdin:
    word, count = line.split('\t')
    count = int(count)
    if word == curr_word:
        curr_count += count
    else:
        if curr_word:
            print '{0}\t{1}'.format(curr_word, curr_count)
        curr_word = word
        curr_count = count

if curr_word == word:
    print '{0}\t{1}'.format(curr_word, curr_count)

To run it:
echo 'jack be nimble jack be quick' | ./mapper.py | sort -t 1 | ./reducer.py

Run on Hadoop:
$HADOOP_HOME/bin/hadoop jar
$HADOOP_HOME/mapred/contrib/streaming/hadoop-streaming*.jar -files mapper.py,reducer.py -mapper mapper.py -reducer reducer.py -input /user/hduser/input.txt -output user/hduser/output

PIG
Load using function -> parser
Transform (GROUP ,FILTER)
C = FOREACH B GENERATE group, A.name;
STORE

With Python:
REGISTER 'udfs/my_first_udf.py' USING streaming_python AS pyudfs;
relation = FOREACH data GENERATE my_udf.function(field);
B = FOREACH A GENERATE pyudfs.return_one();

Local mode: local file system
Map reduce mode: requires Hadoop

Spark

Spark is known: Resilient Distributed Datasets (RDDs). RDDs are
collections of elements partitioned across the nodes of the cluster
that can be operated on in parallel. 100x faster than map reduce because runs in memory in a cluster

from pyspark import SparkContext
def main():
sc = SparkContext(appName='SparkWordCount')
input_file = sc.textFile('/user/hduser/input/input.txt')
counts = input_file.flatMap(lambda line: line.split()) \
.map(lambda word: (word, 1)) \
.reduceByKey(lambda a, b: a + b)
counts.saveAsTextFile('/user/hduser/output')
sc.stop()
if __name__ == '__main__':
main()

Resilient Distributed Datasets (RDDs) are the fundamental programming
abstraction in Spark.

>>> data = [1, 2, 3, 4, 5]
>>> rdd = sc.parallelize(data, 4) # Number of parallel partitions
>>> rdd.glom().collect()
...
[[1, 2, 3, 4, 5]]
The RDD.glom() method returns a list of all of the elements within
each partition, and the RDD.collect() method brings all the elements
to the driver node.

>>> lines = sc.textFile('data.txt')
>>> line_lengths = lines.map(lambda x: len(x))
>>> document_length = line_lengths.reduce(lambda x,y: x+y)
>>> print document_length

The first statement creates an RDD from the external file data.txt.
This file is not loaded at this point; the variable lines is just a
pointer to the external source. The second statement performs a
transformation on the base RDD by using the map() function to calculate
the number of characters in each line. The variable
line_lengths is not immediately computed due to the laziness of
transformations. Finally, the reduce() method is called, which is an
action. At this point, Spark divides the computations into tasks to
run on separate machines. Each machine runs both the map and
reduction on its local data, returning only the results to the driver
program.
If the application were to use line_lengths again, it would be best
to persist the result of the map transformation to ensure that the
map would not be recomputed. The following line will save
line_lengths into memory after the first time it is computed:
>>> line_lengths.persist()


Workflow
Luigi

Each task declares its dependencies on targets created by other tasks.
This enables Luigi to create dependency chains that ensure a task
will not be executed until all of the dependent tasks and all of the
dependencies for those tasks are satisfied.

GCP
Installed app-engine-python
gcloud app deploy -> select region
This creates an app instance

Django deploy. Went well until cx_Oracle issue.

Big Query
bq show publicdata.samples.gsod
Shows the definition (schema)
bq query "select from where order by"
bq mk babynames - creates a new dataset. It is project_id:dataset_id.table

bq load babynames.names2010 yob2010.txt name:string,gender:string,count:integer
The bq load command arguments:
datasetID: babynames
tableID: names2010
source: yob2010.txt
schema: name:string,gender:string,count:integer

bq rm -r babynames


Big Query - BigTable

BigQuery is really for OLAP type of query and scan large amount of data and is not designed for OLTP type queries. For small read/writes, it takes about 2 seconds while BigTable takes about 9 milliseconds for the same amount of data. 
BigQuery is a query Engine for datasets that don't change much, or change by appending. It's a great choice when your queries require a "table scan" or the need to look across the entire database. Think sums, averages, counts, groupings. BigQuery is what you use when you have collected a large amount of data, and need to ask questions about it.

BigTable has no SQL interface and you can only use API go Put/Get/Delete individual rows or run scan operations. BigTable can be easily integrated with other GCP tools
BigTable is much better off for OLTP type of queries.
BigTable is a database. It is designed to be the foundation for a large, scaleable application. Use BigTable when you are making any kind of app that needs to read and write data, and scale is a potential issue. 

Pub/Sub
Publisher - Subscriber
Publisher sends message for a topic. Until digested they are stored in a MSG storage.
Subscriber subscribes and digests the message - sends ACK so that message can be removed from queue



Barclays Bank Plc.
Wokingham Branch
Bracknell Group P O Box 61
Bracknell
Berks
RG12 1GJ

SWIFT: BARCGB22
IBAN: GB94BARC20117487715933

CIB Bank
1027 Budapest
Medve u. 4-14
 
IBAN: HU57107003477030171750000005
SWIFT: CIBHHUHB

00 48 605 880 966
